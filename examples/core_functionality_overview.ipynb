{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495d454a",
   "metadata": {},
   "source": [
    "# szia-ai-tools: Core Functionality\n",
    "This notebook demonstrates how to use the core functions and classes when developing components and applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4e32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload imports\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c460f685",
   "metadata": {},
   "source": [
    "The example locations include file paths used to demonstrate how to process locally stored documents. You can add your own PDF, TXT, MD, or HTML files to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2035ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_urls = [\n",
    "    \"https://www.lynxanalytics.com/generative-ai-platform\",\n",
    "    \"https://en.wikipedia.org/wiki/GPT-3\",\n",
    "    \"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\",\n",
    "    \"https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\",\n",
    "    \"https://en.wikipedia.org/wiki/Large_language_model\",\n",
    "    \"https://arxiv.org/pdf/1706.03762\",\n",
    "]\n",
    "\n",
    "example_locations = [\n",
    "    '../data/example_doc_to_parse/ChatGPT - Wikipedia.html', \n",
    "    '../README.md', \n",
    "    '../data/example_doc_to_parse/wiki_GPT2.txt', \n",
    "    '../data/example_doc_to_parse/2402.06196.pdf'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f80e45",
   "metadata": {},
   "source": [
    "## The DocumentLibrary Class\n",
    "\n",
    "The `DocumentLibrary` is a utility for ingesting and processing documents such as PDF, TXT, MD, and HTML files. It can be initialized with a list of `Document` instances or directly via local file paths and/or URLs. When URLs are provided, the library attempts to crawl and retrieve their content (with more advanced crawling planned in the future). All documents are parsed and converted into standardized Markdown format before being chunked. Two chunking strategies are available: `simple`, which splits text by token count, and `simple_intervals`, which respects structural markers (e.g., headings, paragraphs) defined in `text_split_order`, aiming for clean, context-aware chunks within specified size limits. Metadata—such as source and error status—is tracked throughout processing, and sources can be manually enabled or disabled. The processed chunks can be exported as a `pandas.DataFrame` or a Python list (with metadata), making the `DocumentLibrary` ready for integration with downstream RAG pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8f7a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sziaaitls.core.document_library import DocumentLibrary\n",
    "\n",
    "mydoclib = await DocumentLibrary.create(\n",
    "    urls=example_urls, # can be empty\n",
    "    file_paths=example_locations, # can be empty\n",
    "    chunk_method=\"simple_intervals\",\n",
    "    max_chunk_size=768,\n",
    "    min_chunk_size=256,\n",
    "    min_leftover_chunk_size=128,\n",
    "    ignore_single_newline=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f737ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoclib.documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ddb7c9",
   "metadata": {},
   "source": [
    "Instances of the `Document` class contain an internal variable called page_content. While each Document can parse and chunk itself, the `DocumentLibrary` is designed to handle these operations collectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b763e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_document = mydoclib.documents[0]\n",
    "print(first_document.metadata)  # print metadata of the first document\n",
    "\n",
    "print(type(first_document))  # there are MarkdownDocument, HtmlDocument, PdfDocument, TextDocument classes\n",
    "print(first_document.page_content[:20]) \n",
    "\n",
    "parsed_doc_content = await first_document.get_parsed_content()\n",
    "print(parsed_doc_content[:100]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f795c90",
   "metadata": {},
   "source": [
    "The `get_chunks_with_metadata` function returns all chunks along with their associated metadata. Each Document instance retains its parsed and chunked content after the initial processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "_chs = await mydoclib.get_chunks_with_metadata()\n",
    "_chs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab436e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pandas DataFrame export\n",
    "pdf_chunks = await mydoclib.get_chunk_df()\n",
    "pdf_chunks.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d517c",
   "metadata": {},
   "source": [
    "The `DocumentLibrary`'s metadata stores key information about each contained document at the URI level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e49f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoclib.get_metadata_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab7b9e",
   "metadata": {},
   "source": [
    "We can visualize a histogram of the chunk sizes to verify whether they fall within the specified minimum and maximum token limits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb021f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_chunks.chunk_token_size.plot(kind='hist', bins=50, title='PDF Chunk Token Size Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb511f",
   "metadata": {},
   "source": [
    "## The TextEmbedder Class\n",
    "\n",
    "The `TextEmbedder` is a utility class that converts chunks of text into vector embeddings, which are useful for semantic search, clustering, or RAG (Retrieval-Augmented Generation) pipelines. It supports multiple backends - for example, `\"openai\"` for hosted models or `\"ollama\"` for local inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b558b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sziaaitls.core.embedders import TextEmbedder\n",
    "openai_embedder = TextEmbedder(\"openai\", api_key=os.environ.get(\"OPENAI_API_KEY\"), model=\"text-embedding-3-small\", rate_limit=3000, period=60, max_batch_tokens=32768)\n",
    "pdf_chunks['content_embedding'] = await openai_embedder.acreate(\n",
    "    pdf_chunks['chunk_content'].tolist())\n",
    "pdf_chunks.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca06f72",
   "metadata": {},
   "source": [
    "To use the Ollama embedder, ensure that Ollama is installed and running (on Linux/Mac, run `ollama serve` in the terminal). Also, make sure you have downloaded the desired model by running `ollama pull nomic-embed-text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98076995",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_embedder = TextEmbedder(\"ollama\", model=\"nomic-embed-text\")\n",
    "pdf_chunks['content_embedding'] = await ollama_embedder.acreate(\n",
    "    pdf_chunks['chunk_content'].tolist())\n",
    "pdf_chunks.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3ca294",
   "metadata": {},
   "source": [
    "### The LLMPromptProcessor Class\n",
    "\n",
    "The `LLMPromptProcessor` class provides a unified interface for sending prompts to language models, supporting both OpenAI and Ollama-compatible models. It is designed for flexibility and ease of integration across different backends.\n",
    "\n",
    "It accepts a `ChatCompletionPrompt`—which is a structured list of `Message` objects (each representing a role: `system`, `user`, or `assistant`)—and returns a single `Message` as the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sziaaitls.core.llms import LLMPromptProcessor, Message, ChatCompletionPrompt\n",
    "prompt_text = [\n",
    "    {'role':'system', 'content': 'You are a robot that tells a joke about the topic the user mentions.'}, \n",
    "    {'role':'user', 'content': 'Tell me a joke about cats.'},\n",
    "]\n",
    "llm = LLMPromptProcessor(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    rate_limit = 350,\n",
    "    period = 60,\n",
    ")\n",
    "\n",
    "prompt = ChatCompletionPrompt([Message.from_dict(m) for m in prompt_text])\n",
    "answer = await llm.acreate(prompt)\n",
    "\n",
    "answer.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ee755",
   "metadata": {},
   "source": [
    "If you want to use it with Ollama, make sure that Ollama is installed and running (on Linux/Mac, run `ollama serve` in the terminal). Also, ensure that the selected model is running—for example, by executing `ollama run gemma3:4b` in another terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3b9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ollama = LLMPromptProcessor(\n",
    "    api_key='ollama',\n",
    "    model=\"gemma3:4b\",\n",
    "    base_url = \"http://localhost:11434/v1\",\n",
    "    rate_limit = 350,\n",
    "    period = 60,\n",
    ")\n",
    "\n",
    "prompt = ChatCompletionPrompt([Message.from_dict(m) for m in prompt_text])\n",
    "answer = await llm_ollama.acreate(prompt)\n",
    "\n",
    "answer.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c5e80",
   "metadata": {},
   "source": [
    "## Connecting to Vector Databases (VDB)\n",
    "The `VectorStore` class is currently limited in functionality—it supports only storing and retrieving embeddings. You can upsert items using the add function and retrieve similar items using the search function. Currently, it supports [FAISS](https://github.com/facebookresearch/faiss) and [USearch](https://github.com/unum-cloud/usearch) as vector databases.\n",
    "\n",
    "The add function expects a list of `Embedding` objects, while the search function returns a list of `EmbeddingSimilarity` results, each of which includes an `Embedding` and its cosine similarity score.\n",
    "\n",
    "The `Embedding` class holds the content (id), a unique ID (embedding_content), the embedding vector values (embedding_value), and optional metadata (metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sziaaitls.core.embedders import Embedding\n",
    "from sziaaitls.core.vector_stores import VectorStore\n",
    "row1 = pdf_chunks.iloc[0].to_dict()\n",
    "row2 = pdf_chunks.iloc[1].to_dict()\n",
    "row3= pdf_chunks.iloc[2].to_dict()\n",
    "\n",
    "embedding_list = [\n",
    "    Embedding(id=0, embedding_content=row1['chunk_content'], embedding_value=row1['content_embedding'], \n",
    "              embedding_metadata={'uri':row1['uri'], 'chunk_order':row1['chunk_order']}), \n",
    "    Embedding(id=1, embedding_content=row2['chunk_content'], embedding_value=row2['content_embedding'], \n",
    "              embedding_metadata={'uri':row2['uri'], 'chunk_order':row2['chunk_order']}),\n",
    "    Embedding(id=2, embedding_content=row3['chunk_content'], embedding_value=row3['content_embedding'], \n",
    "              embedding_metadata={'uri':row3['uri'], 'chunk_order':row3['chunk_order']}),\n",
    "    ]\n",
    "\n",
    "vector_store = VectorStore(name=\"usearch\", dimension=1536)\n",
    "await vector_store.add(embedding_list)\n",
    "\n",
    "results = await vector_store.search(row2['content_embedding'], k=3)\n",
    "[(res.embedding.id, res.similarity) for res in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a0eb1",
   "metadata": {},
   "source": [
    "## The SimpleRAG Class\n",
    "\n",
    "The `SimpleRAG` class connects a `DocumentLibrary` and a `VectorStore` to create a minimal Retrieval-Augmented Generation (RAG) pipeline.\n",
    "\n",
    "It allows you to:\n",
    "- Build a vector index by embedding and storing document chunks.\n",
    "- Perform similarity-based searches against this index using natural language queries.\n",
    "\n",
    "The class supports both OpenAI and Ollama-compatible models for embedding and can use FAISS or USearch as the vector backend. The `create()` class method initializes and populates the index from a `DocumentLibrary`. The `search_query()` method retrieves relevant chunks with optional filtering by token limits, returning ranked results with cosine similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing RAG solutions\n",
    "from sziaaitls.core.rag_solutions import SimpleRAG\n",
    "from sziaaitls.core.vector_stores import VectorStore\n",
    "from sziaaitls.core.embedders import TextEmbedder\n",
    "\n",
    "# initializing a vector store and an embedder\n",
    "vector_store = VectorStore(name=\"usearch\", dimension=1536)\n",
    "embedder = TextEmbedder(\"openai\", api_key=os.environ.get(\"OPENAI_API_KEY\"), model=\"text-embedding-3-small\", rate_limit=3000, period=60, max_batch_tokens=32768)\n",
    "\n",
    "# creating a SimpleRAG instance\n",
    "simple_rag = await SimpleRAG.create(mydoclib, embedder, vector_store) # using the document library from above\n",
    "print(f\"RAG Store is created with {simple_rag.next_id -1} items.\")\n",
    "\n",
    "\n",
    "pd.DataFrame(simple_rag.metadata_list).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc59520",
   "metadata": {},
   "outputs": [],
   "source": [
    "findings = await simple_rag.search_query(\"What are transformers used for?\", max_results=2)\n",
    "\n",
    "for i in range(len(findings)):\n",
    "    print(\n",
    "        f\"{findings[i].embedding.metadata['uri']}, \\\n",
    "        chunk: {findings[i].embedding.metadata['chunk_order']}, \\\n",
    "        similarity: {findings[i].similarity} \\\n",
    "        \\ncontent: {findings[i].embedding.embedding_content}\\n\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed55b7",
   "metadata": {},
   "source": [
    "**F. I. N.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

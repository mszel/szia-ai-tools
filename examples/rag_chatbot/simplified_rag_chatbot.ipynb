{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e67a86",
   "metadata": {},
   "source": [
    "# Examples of Combining Core Modules\n",
    "\n",
    "By combining `LLMPromptProcessor` with RAG components, you can create RAG-based chatbots. The example below is not fully optimized but serves to illustrate how RAG chatbots work in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50140200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload imports\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aecab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_urls = [\n",
    "    \"https://www.lynxanalytics.com/generative-ai-platform\",\n",
    "    \"https://en.wikipedia.org/wiki/GPT-3\",\n",
    "    \"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\",\n",
    "    \"https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\",\n",
    "    \"https://en.wikipedia.org/wiki/Large_language_model\",\n",
    "    \"https://arxiv.org/pdf/1706.03762\",\n",
    "]\n",
    "\n",
    "example_locations = [\n",
    "    '../../data/example_doc_to_parse/ChatGPT - Wikipedia.html', \n",
    "    '../../README.md', \n",
    "    '../../data/example_doc_to_parse/wiki_GPT2.txt', \n",
    "    '../../data/example_doc_to_parse/2402.06196.pdf'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de166f5",
   "metadata": {},
   "source": [
    "You can replace the `{context}` placeholder with the top retrieved content to create a simple RAG-style chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626dacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sziaaitls.core.document_library import DocumentLibrary\n",
    "from sziaaitls.core.rag_solutions import SimpleRAG\n",
    "from sziaaitls.core.vector_stores import VectorStore\n",
    "from sziaaitls.core.embedders import TextEmbedder\n",
    "from sziaaitls.core.llms import LLMPromptProcessor, Message, ChatCompletionPrompt\n",
    "\n",
    "mydoclib = await DocumentLibrary.create(\n",
    "    urls=example_urls, # can be empty\n",
    "    file_paths=example_locations, # can be empty\n",
    "    chunk_method=\"simple_intervals\",\n",
    "    max_chunk_size=768,\n",
    "    min_chunk_size=256,\n",
    "    min_leftover_chunk_size=128,\n",
    "    ignore_single_newline=True,\n",
    ")\n",
    "\n",
    "# initializing a vector store and an embedder\n",
    "vector_store = VectorStore(name=\"usearch\", dimension=1536)\n",
    "embedder = TextEmbedder(\"openai\", api_key=os.environ.get(\"OPENAI_API_KEY\"), model=\"text-embedding-3-small\", rate_limit=3000, period=60, max_batch_tokens=32768)\n",
    "\n",
    "# creating a SimpleRAG instance\n",
    "simple_rag = await SimpleRAG.create(mydoclib, embedder, vector_store) # using the document library from above\n",
    "print(f\"RAG Store is created with {simple_rag.next_id -1} items.\")\n",
    "\n",
    "\n",
    "pd.DataFrame(simple_rag.metadata_list).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant that answers questions based on the provided context.'},\n",
    "    {'role': 'system', 'content': 'Please phrase your answer solely based on the following context:\\n\\n{context}.'},\n",
    "    {'role': 'user', 'content': '{question}'}\n",
    "]\n",
    "\n",
    "async def RAG_based_answer(\n",
    "        question: str, \n",
    "        rag_kb: SimpleRAG, \n",
    "        chatbot_prompt: list[dict], \n",
    "        max_context_size_token: int=2000, \n",
    "        max_context_size_chunks: int=20) -> str:\n",
    "\n",
    "    context_es = await rag_kb.search_query(question, max_results=max_context_size_chunks, token_limit=max_context_size_token)\n",
    "    context_txt = \"\\n\\n--------------------------------------\\n\\n\".join([finding.embedding.embedding_content for finding in context_es])\n",
    "    \n",
    "    prompt = ChatCompletionPrompt([Message.from_dict(p).format(question=question, context=context_txt) for p in chatbot_prompt])\n",
    "\n",
    "    llm = LLMPromptProcessor(\n",
    "        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "        model=\"gpt-4o-mini\",\n",
    "        rate_limit = 350,\n",
    "        period = 60,\n",
    "    )\n",
    "\n",
    "    answer = await llm.acreate(prompt)\n",
    "\n",
    "    return answer.content\n",
    "    \n",
    "# Example question\n",
    "question = \"What is the main idea behind GPT-3?\"\n",
    "answer = await RAG_based_answer(\n",
    "    question=question, \n",
    "    rag_kb=simple_rag, \n",
    "    chatbot_prompt=prompt_list, \n",
    "    max_context_size_token=2000, \n",
    "    max_context_size_chunks=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Question: {question}\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3166b89a",
   "metadata": {},
   "source": [
    "**F.I.N**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
